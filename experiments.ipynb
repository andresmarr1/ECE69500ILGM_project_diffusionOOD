{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments using DDPM for OOD"
      ],
      "metadata": {
        "id": "cdwlbnen5mtN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from typing import List, Dict, Optional, Tuple, Union\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST, FashionMNIST\n",
        "from torch.utils.data import DataLoader\n",
        "import random\n",
        "import math"
      ],
      "metadata": {
        "id": "42BgN-QC5mHs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##DDPM model"
      ],
      "metadata": {
        "id": "HLEy4KNE6M8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "qiZVb_JP2v9i"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeEmbedding(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Embeddings for $t$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int):\n",
        "        \"\"\"\n",
        "        * `n_channels` is the number of dimensions in the embedding\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.n_channels = n_channels\n",
        "        # First linear layer\n",
        "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
        "        # Activation\n",
        "        self.act = nn.SiLU()\n",
        "        # Second linear layer\n",
        "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
        "\n",
        "    def forward(self, t: torch.Tensor):\n",
        "        # Create sinusoidal position embeddings\n",
        "        # [same as those from the transformer](../../transformers/positional_encoding.html)\n",
        "        #\n",
        "        # \\begin{align}\n",
        "        # PE^{(1)}_{t,i} &= sin\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg) \\\\\n",
        "        # PE^{(2)}_{t,i} &= cos\\Bigg(\\frac{t}{10000^{\\frac{i}{d - 1}}}\\Bigg)\n",
        "        # \\end{align}\n",
        "        #\n",
        "        # where $d$ is `half_dim`\n",
        "        half_dim = self.n_channels // 8\n",
        "        emb = math.log(10_000) / (half_dim - 1)\n",
        "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
        "        emb = t[:, None] * emb[None, :]\n",
        "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
        "\n",
        "        # Transform with the MLP\n",
        "        emb = self.act(self.lin1(emb))\n",
        "        emb = self.lin2(emb)\n",
        "\n",
        "        #\n",
        "        return emb\n",
        "\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Residual block\n",
        "\n",
        "    A residual block has two convolution layers with group normalization.\n",
        "    Each resolution is processed with two residual blocks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int,\n",
        "                 n_groups: int = 2, dropout: float = 0.5):\n",
        "        \"\"\"\n",
        "        * `in_channels` is the number of input channels\n",
        "        * `out_channels` is the number of input channels\n",
        "        * `time_channels` is the number channels in the time step ($t$) embeddings\n",
        "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
        "        * `dropout` is the dropout rate\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        # Group normalization and the first convolution layer\n",
        "        self.norm1 = nn.GroupNorm(n_groups, in_channels)\n",
        "        self.act1 = nn.SiLU()\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        # Group normalization and the second convolution layer\n",
        "        self.norm2 = nn.GroupNorm(n_groups, out_channels)\n",
        "        self.act2 = nn.SiLU()\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        # If the number of input channels is not equal to the number of output channels we have to\n",
        "        # project the shortcut connection\n",
        "        if in_channels != out_channels:\n",
        "            self.shortcut = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "\n",
        "        # Linear layer for time embeddings\n",
        "        self.time_emb = nn.Linear(time_channels, out_channels)\n",
        "        self.time_act = nn.SiLU()\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
        "        * `t` has shape `[batch_size, time_channels]`\n",
        "        \"\"\"\n",
        "        # First convolution layer\n",
        "        h = self.conv1(self.act1(self.norm1(x)))\n",
        "        # Add time embeddings\n",
        "        h += self.time_emb(self.time_act(t))[:, :, None, None]\n",
        "        # Second convolution layer\n",
        "        h = self.conv2(self.dropout(self.act2(self.norm2(h))))\n",
        "\n",
        "        # Add the shortcut connection and return\n",
        "        return h + self.shortcut(x)\n",
        "\n",
        "class AttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Attention block\n",
        "\n",
        "    This is similar to [transformer multi-head attention](../../transformers/mha.html).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int, n_heads: int = 1, d_k: int = None, n_groups: int = 4):\n",
        "        \"\"\"\n",
        "        * `n_channels` is the number of channels in the input\n",
        "        * `n_heads` is the number of heads in multi-head attention\n",
        "        * `d_k` is the number of dimensions in each head\n",
        "        * `n_groups` is the number of groups for [group normalization](../../normalization/group_norm/index.html)\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Default `d_k`\n",
        "        if d_k is None:\n",
        "            d_k = n_channels\n",
        "        # Normalization layer\n",
        "        self.norm = nn.GroupNorm(n_groups, n_channels)\n",
        "        # Projections for query, key and values\n",
        "        self.projection = nn.Linear(n_channels, n_heads * d_k * 3)\n",
        "        # Linear layer for final transformation\n",
        "        self.output = nn.Linear(n_heads * d_k, n_channels)\n",
        "        # Scale for dot-product attention\n",
        "        self.scale = d_k ** -0.5\n",
        "        #\n",
        "        self.n_heads = n_heads\n",
        "        self.d_k = d_k\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: Optional[torch.Tensor] = None):\n",
        "        \"\"\"\n",
        "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
        "        * `t` has shape `[batch_size, time_channels]`\n",
        "        \"\"\"\n",
        "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
        "        # to match with `ResidualBlock`.\n",
        "        _ = t\n",
        "        # Get shape\n",
        "        batch_size, n_channels, height, width = x.shape\n",
        "        # Change `x` to shape `[batch_size, seq, n_channels]`\n",
        "        x = x.view(batch_size, n_channels, -1).permute(0, 2, 1)\n",
        "        # Get query, key, and values (concatenated) and shape it to `[batch_size, seq, n_heads, 3 * d_k]`\n",
        "        qkv = self.projection(x).view(batch_size, -1, self.n_heads, 3 * self.d_k)\n",
        "        # Split query, key, and values. Each of them will have shape `[batch_size, seq, n_heads, d_k]`\n",
        "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
        "        # Calculate scaled dot-product $\\frac{Q K^\\top}{\\sqrt{d_k}}$\n",
        "        attn = torch.einsum('bihd,bjhd->bijh', q, k) * self.scale\n",
        "        # Softmax along the sequence dimension $\\underset{seq}{softmax}\\Bigg(\\frac{Q K^\\top}{\\sqrt{d_k}}\\Bigg)$\n",
        "        attn = attn.softmax(dim=2)\n",
        "        # Multiply by values\n",
        "        res = torch.einsum('bijh,bjhd->bihd', attn, v)\n",
        "        # Reshape to `[batch_size, seq, n_heads * d_k]`\n",
        "        res = res.view(batch_size, -1, self.n_heads * self.d_k)\n",
        "        # Transform to `[batch_size, seq, n_channels]`\n",
        "        res = self.output(res)\n",
        "\n",
        "        # Add skip connection\n",
        "        res += x\n",
        "\n",
        "        # Change to shape `[batch_size, in_channels, height, width]`\n",
        "        res = res.permute(0, 2, 1).view(batch_size, n_channels, height, width)\n",
        "\n",
        "        #\n",
        "        return res\n",
        "\n",
        "\n",
        "class DownBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Down block\n",
        "\n",
        "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the first half of U-Net at each resolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        self.res = ResidualBlock(in_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res(x, t)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class UpBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Up block\n",
        "\n",
        "    This combines `ResidualBlock` and `AttentionBlock`. These are used in the second half of U-Net at each resolution.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, in_channels: int, out_channels: int, time_channels: int, has_attn: bool):\n",
        "        super().__init__()\n",
        "        # The input has `in_channels + out_channels` because we concatenate the output of the same resolution\n",
        "        # from the first half of the U-Net\n",
        "        self.res = ResidualBlock(in_channels + out_channels, out_channels, time_channels)\n",
        "        if has_attn:\n",
        "            self.attn = AttentionBlock(out_channels)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res(x, t)\n",
        "        x = self.attn(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class MiddleBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Middle block\n",
        "\n",
        "    It combines a `ResidualBlock`, `AttentionBlock`, followed by another `ResidualBlock`.\n",
        "    This block is applied at the lowest resolution of the U-Net.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels: int, time_channels: int):\n",
        "        super().__init__()\n",
        "        self.res1 = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "        self.attn = AttentionBlock(n_channels)\n",
        "        self.res2 = ResidualBlock(n_channels, n_channels, time_channels)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        x = self.res1(x, t)\n",
        "        x = self.attn(x)\n",
        "        x = self.res2(x, t)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Scale up the feature map by $2 \\times$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.ConvTranspose2d(n_channels, n_channels, (4, 4), (2, 2), (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
        "        # to match with `ResidualBlock`.\n",
        "        _ = t\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Downsample(nn.Module):\n",
        "    \"\"\"\n",
        "    ### Scale down the feature map by $\\frac{1}{2} \\times$\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_channels):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(n_channels, n_channels, (3, 3), (2, 2), (1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        # `t` is not used, but it's kept in the arguments because for the attention layer function signature\n",
        "        # to match with `ResidualBlock`.\n",
        "        _ = t\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    \"\"\"\n",
        "    ## U-Net\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, image_channels: int = 1, n_channels: int = 16,\n",
        "                 ch_mults: Union[Tuple[int, ...], List[int]] = (1, 2, 2, 4),\n",
        "                 is_attn: Union[Tuple[bool, ...], List[bool]] = (False, False, True, True),\n",
        "                 n_blocks: int = 2):\n",
        "        \"\"\"\n",
        "        * `image_channels` is the number of channels in the image. $3$ for RGB.\n",
        "        * `n_channels` is number of channels in the initial feature map that we transform the image into\n",
        "        * `ch_mults` is the list of channel numbers at each resolution. The number of channels is `ch_mults[i] * n_channels`\n",
        "        * `is_attn` is a list of booleans that indicate whether to use attention at each resolution\n",
        "        * `n_blocks` is the number of `UpDownBlocks` at each resolution\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        # Number of resolutions\n",
        "        n_resolutions = len(ch_mults)\n",
        "\n",
        "        # Project image into feature map\n",
        "        self.image_proj = nn.Conv2d(image_channels, n_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "        # Time embedding layer. Time embedding has `n_channels * 4` channels\n",
        "        self.time_emb = TimeEmbedding(n_channels * 4)\n",
        "\n",
        "        # #### First half of U-Net - decreasing resolution\n",
        "        down = []\n",
        "        # Number of channels\n",
        "        out_channels = in_channels = n_channels\n",
        "        # For each resolution\n",
        "        for i in range(n_resolutions):\n",
        "            # Number of output channels at this resolution\n",
        "            out_channels = in_channels * ch_mults[i]\n",
        "            # Add `n_blocks`\n",
        "            for _ in range(n_blocks):\n",
        "                down.append(DownBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "                in_channels = out_channels\n",
        "            # Down sample at all resolutions except the last\n",
        "            if i < n_resolutions - 1:\n",
        "                down.append(Downsample(in_channels))\n",
        "\n",
        "        # Combine the set of modules\n",
        "        self.down = nn.ModuleList(down)\n",
        "\n",
        "        # Middle block\n",
        "        self.middle = MiddleBlock(out_channels, n_channels * 4, )\n",
        "\n",
        "        # #### Second half of U-Net - increasing resolution\n",
        "        up = []\n",
        "        # Number of channels\n",
        "        in_channels = out_channels\n",
        "        # For each resolution\n",
        "        for i in reversed(range(n_resolutions)):\n",
        "            # `n_blocks` at the same resolution\n",
        "            out_channels = in_channels\n",
        "            for _ in range(n_blocks):\n",
        "                up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "            # Final block to reduce the number of channels\n",
        "            out_channels = in_channels // ch_mults[i]\n",
        "            up.append(UpBlock(in_channels, out_channels, n_channels * 4, is_attn[i]))\n",
        "            in_channels = out_channels\n",
        "            # Up sample at all resolutions except last\n",
        "            if i > 0:\n",
        "                up.append(Upsample(in_channels))\n",
        "\n",
        "        # Combine the set of modules\n",
        "        self.up = nn.ModuleList(up)\n",
        "\n",
        "        # Final normalization and convolution layer\n",
        "        self.norm = nn.GroupNorm(8, n_channels)\n",
        "        self.act = nn.SiLU()\n",
        "        self.final = nn.Conv2d(in_channels, image_channels, kernel_size=(3, 3), padding=(1, 1))\n",
        "\n",
        "    def forward(self, x: torch.Tensor, t: torch.Tensor):\n",
        "        \"\"\"\n",
        "        * `x` has shape `[batch_size, in_channels, height, width]`\n",
        "        * `t` has shape `[batch_size]`\n",
        "        \"\"\"\n",
        "\n",
        "        # Get time-step embeddings\n",
        "        t = self.time_emb(t)\n",
        "\n",
        "        # Get image projection\n",
        "        x = self.image_proj(x)\n",
        "\n",
        "        # `h` will store outputs at each resolution for skip connection\n",
        "        h = [x]\n",
        "        # First half of U-Net\n",
        "        for m in self.down:\n",
        "            x = m(x, t)\n",
        "            h.append(x)\n",
        "\n",
        "        # Middle (bottom)\n",
        "        x = self.middle(x, t)\n",
        "\n",
        "        # Second half of U-Net\n",
        "        for m in self.up:\n",
        "            if isinstance(m, Upsample):\n",
        "                x = m(x, t)\n",
        "            else:\n",
        "                # Get the skip connection from first half of U-Net and concatenate\n",
        "                s = h.pop()\n",
        "                x = torch.cat((x, s), dim=1)\n",
        "                #\n",
        "                x = m(x, t)\n",
        "\n",
        "        # Final normalization and convolution\n",
        "        return self.final(self.act(self.norm(x)))"
      ],
      "metadata": {
        "id": "wi9rUY5e6YTX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DiffusionModel(nn.Module):\n",
        "  def __init__(self,model,steps,s_val,image_shape,channels):\n",
        "    super(DiffusionModel, self).__init__()\n",
        "    self.model = model\n",
        "    self.steps = steps\n",
        "    self.s_val = s_val\n",
        "    self.image_shape = image_shape\n",
        "    self.channels = channels\n",
        "    self.beta = self.beta_schedule(steps,s_val)\n",
        "    self.alphas = 1-self.beta\n",
        "    self.alphas_cumprod = np.cumprod(self.alphas, axis = -1)\n",
        "  #cosine noise scheduler\n",
        "  def beta_schedule(self, steps, s_val):\n",
        "    def f(t):\n",
        "      return np.cos((t/steps+s_val)/(1+s_val)*0.5*torch.pi)**2\n",
        "    x = np.linspace(0,steps,steps+1)\n",
        "    alphas_cumproduct = f(x)/f(np.array([0]))\n",
        "    beta_ = 1 - alphas_cumproduct[1:]/alphas_cumproduct[:-1]\n",
        "    beta_ = np.clip(beta_,0.0001,0.999)\n",
        "    return beta_\n",
        "\n",
        "\n",
        "  def forward_diffusion(self, image,step):\n",
        "    try:\n",
        "      step = step.to('cpu')\n",
        "    except:\n",
        "      pass\n",
        "    sqrt_alphacumprod = torch.Tensor(np.sqrt(self.alphas_cumprod[step])).view(-1,1,1,1).to(device)\n",
        "    sqrt_oneminus_alphacumprod = torch.Tensor(np.sqrt(1-self.alphas_cumprod[step])).view(-1,1,1,1).to(device)\n",
        "    epsilon = torch.randn_like(image).to(device)\n",
        "    noised_image = sqrt_alphacumprod*image.to(device) + sqrt_oneminus_alphacumprod*epsilon\n",
        "    return noised_image, epsilon\n",
        "\n",
        "  def backward_diffusion(self, noised_img, step):\n",
        "    noise = self.model(noised_img, step)\n",
        "    return noise\n",
        "\n",
        "  def reconstruct_imgs(self, n_img, time):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      denoised_img = n_img.to(device)\n",
        "      for step in tqdm(range(time)):\n",
        "        step = time - step - 1\n",
        "        step = torch.full((1,), step).to(device)\n",
        "        predicted_noise = self.backward_diffusion(denoised_img,step)\n",
        "        alpha_t = self.alphas[step]\n",
        "        cumprod_alpha_t = self.alphas_cumprod[step]\n",
        "\n",
        "        denoised_img = (1/np.sqrt(alpha_t))*(denoised_img - ((1-alpha_t)/np.sqrt(1 - cumprod_alpha_t))* predicted_noise)\n",
        "    self.model.train()\n",
        "    return denoised_img\n",
        "  def generate_samples(self, number_samples):\n",
        "    self.model.eval()\n",
        "    with torch.no_grad():\n",
        "      samples = torch.randn(number_samples, self.channels, self.image_shape, self.image_shape, device = device)\n",
        "\n",
        "      for step in tqdm(range(self.steps)):\n",
        "        step = self.steps - step -1\n",
        "        step = torch.full((number_samples,), step)\n",
        "        predicted_noise = self.backward_diffusion(samples, step.to(device))\n",
        "        alpha_t = self.alphas[step]\n",
        "        alpha_cumprod_t = self.alphas_cumprod[step]\n",
        "        samples = (1 / np.sqrt(alpha_t)) * (samples - (1 - alpha_t) / np.sqrt((1 - alpha_cumprod_t)) * predicted_noise)\n",
        "    self.model.train()\n",
        "    return samples"
      ],
      "metadata": {
        "id": "Gk6Qu7RU6Lix"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Load in the weights"
      ],
      "metadata": {
        "id": "_Uy94Npb5_DV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DiffusionModel(UNet().to(device),1000,0.008,32,1)"
      ],
      "metadata": {
        "id": "IM59Bmm08knC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpBElwyu5gNy",
        "outputId": "d9ecdc5c-bed4-4cc5-858f-29dc6a297900"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('/content/DDPM_weights.pt')) #change directory to the path where DDPM_ood.pt is"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OOD detection experiments"
      ],
      "metadata": {
        "id": "LLHLdKs06RdQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Define in distribution loader and out of distribution loader\n",
        "transform = transforms.Compose([transforms.Resize((32,32)),transforms.ToTensor()])\n",
        "\n",
        "id_dataset = FashionMNIST(root='./data', train=False, download=True, transform=transform)\n",
        "id_loader = DataLoader(id_dataset, batch_size=1, shuffle=True)\n",
        "\n",
        "ood_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "ood_loader = DataLoader(ood_dataset, batch_size=1, shuffle=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsND4Z53Gvwc",
        "outputId": "88761f81-f8e2-453d-83dd-8dc5abca24ed"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26421880/26421880 [00:05<00:00, 5203832.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 29515/29515 [00:00<00:00, 198030.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4422102/4422102 [00:01<00:00, 3689437.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5148/5148 [00:00<00:00, 20702087.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/FashionMNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 15817541.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 417366.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 503: Service Unavailable\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 4454631.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 5216464.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "perceptionnet = torch.hub.load('pytorch/vision:v0.10.0', 'alexnet', pretrained=True)\n",
        "perceptionnet.eval().to(device);\n",
        "preprocess_alexnet = transforms.Compose([\n",
        "    transforms.Resize(224),\n",
        "    #transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSrrWADR5-i_",
        "outputId": "2e88a5ea-376a-4870-b717-1d2d6d43a69d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to /root/.cache/torch/hub/v0.10.0.zip\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/alexnet-owt-7be5be79.pth\" to /root/.cache/torch/hub/checkpoints/alexnet-owt-7be5be79.pth\n",
            "100%|██████████| 233M/233M [00:01<00:00, 150MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()"
      ],
      "metadata": {
        "id": "ZcBs6Lg49DBb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "May take anywhere from 5-7 days to get results for 1000 samples with GPU acceleration."
      ],
      "metadata": {
        "id": "3pYwGQjaHuxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In Distribution FashionMNIST"
      ],
      "metadata": {
        "id": "bRSG5dxrIwzW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = [100,200,400,500,700,999]\n",
        "results_id = []\n",
        "for it in range(1000):\n",
        "    print(it)\n",
        "    aux = []\n",
        "    og_image, _ = next(iter(id_loader))\n",
        "    og_image = og_image.to(device)\n",
        "    og_image_t = og_image.repeat(1,3,1,1)\n",
        "    og_image_t = preprocess_alexnet(og_image_t)\n",
        "\n",
        "    images = og_image.repeat(len(timesteps),1,1,1)\n",
        "    alexnet_pscore = perceptionnet(og_image_t)[0]\n",
        "    noised_images, eps = model.forward_diffusion(images, timesteps)\n",
        "    for idx, img in enumerate(noised_images):\n",
        "\n",
        "        noised_image = img.unsqueeze(0)\n",
        "        #noised_image = noised_image.repeat(10,1,1,1)\n",
        "        recon = model.reconstruct_imgs(noised_image, timesteps[idx])[0]\n",
        "        loss_mse = criterion(og_image,recon.unsqueeze(0))\n",
        "        n_img = recon.unsqueeze(0).repeat(1,3,1,1)\n",
        "        n_img = preprocess_alexnet(n_img)\n",
        "        recon_pscore = perceptionnet(n_img)[0]\n",
        "        loss_per = criterion(recon_pscore,alexnet_pscore)\n",
        "        adict = {'timestep': timesteps[idx],'mse_loss':loss_mse.item(),'perception_loss':loss_per.item(),'sum':loss_mse.item()+loss_per.item()}\n",
        "        #print(adict)\n",
        "        aux.append(adict)\n",
        "\n",
        "    results_id.append(aux)"
      ],
      "metadata": {
        "id": "wc9VrxgPIzHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Out of distribution MNIST"
      ],
      "metadata": {
        "id": "sLJOpIBhIsmH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "timesteps = [100,200,400,500,700,999]\n",
        "results_ood = []\n",
        "for it in range(1000):\n",
        "    print(it)\n",
        "    aux = []\n",
        "    og_image, _ = next(iter(ood_loader))\n",
        "    og_image = og_image.to(device)\n",
        "    og_image_t = og_image.repeat(1,3,1,1)\n",
        "    og_image_t = preprocess_alexnet(og_image_t)\n",
        "\n",
        "    images = og_image.repeat(len(timesteps),1,1,1)\n",
        "    alexnet_pscore = perceptionnet(og_image_t)[0]\n",
        "    noised_images, eps = model.forward_diffusion(images, timesteps)\n",
        "    for idx, img in enumerate(noised_images):\n",
        "\n",
        "        noised_image = img.unsqueeze(0)\n",
        "        #noised_image = noised_image.repeat(10,1,1,1)\n",
        "        recon = model.reconstruct_imgs(noised_image, timesteps[idx])[0]\n",
        "        loss_mse = criterion(og_image,recon.unsqueeze(0))\n",
        "        n_img = recon.unsqueeze(0).repeat(1,3,1,1)\n",
        "        n_img = preprocess_alexnet(n_img)\n",
        "        recon_pscore = perceptionnet(n_img)[0]\n",
        "        loss_per = criterion(recon_pscore,alexnet_pscore)\n",
        "        adict = {'timestep': timesteps[idx],'mse_loss':loss_mse.item(),'perception_loss':loss_per.item(),'sum':loss_mse.item()+loss_per.item()}\n",
        "        #print(adict)\n",
        "        aux.append(adict)\n",
        "\n",
        "    results_ood.append(aux)"
      ],
      "metadata": {
        "id": "6MUTpTK9HXsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Graphs"
      ],
      "metadata": {
        "id": "mtCBd0PMyoL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "ImXtF0yXypZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for j in range(len(results_ood[0])):\n",
        "  ood_mse_loss100 = []\n",
        "  for i in results_ood:\n",
        "    ood_mse_loss100.append(i[j]['sum'])\n",
        "  id_mse_loss100 = []\n",
        "  for i in results_id:\n",
        "    id_mse_loss100.append(i[j]['sum'])\n",
        "  plt.figure()\n",
        "  plt.title('T = '+str(results_ood[0][j]['timestep']))\n",
        "  plt.xlabel('Reconstruction score')\n",
        "  sns.histplot(data=id_mse_loss100,stat='density',color='blue',label='ID FashionMNIST')\n",
        "  sns.histplot(data=ood_mse_loss100,stat='density',color='red',label='OOD MNIST')\n",
        "  plt.legend()\n"
      ],
      "metadata": {
        "id": "2qaS3aWSyqGX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}